{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f04f8d51",
   "metadata": {},
   "source": [
    "The first thing we will need to do is load the tensorflow package. \"tf\" is usually used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25c16e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e426c35d",
   "metadata": {},
   "source": [
    "Tensorflow has its own versions of typical objects (floats, ints, booleans, etc.). The most important ones are (of course) tensors, which mostly function similar to numpy arrays. They come in two types, \"tf.constant\" and \"tf.Variable\". They have an N1 x N2 x N3... type shape, represented as [N1,N2,N3,..]. No triangular tensors are permitted, for example. Sparse tensors exist, but they're not particularly well supported. By default, everything is float32. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ee79d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "constant_tensor = tf.constant([1.,2.,3.])\n",
    "variable_tensor = tf.Variable([[1.,2.],[3.,4.]])\n",
    "float64_tensor = tf.constant([2.,4.,5.],dtype=\"float64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3251514",
   "metadata": {},
   "source": [
    "We access the shape with .shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13cde2b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([3])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constant_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f92beeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 2])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variable_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931871b0",
   "metadata": {},
   "source": [
    "When tensors have the same shape, multiplication and addition work componentwise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10b2b0bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[4., 6.],\n",
       "       [8., 5.]], dtype=float32)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = tf.constant([[1.,2.],[3.,4.]])\n",
    "x2 = tf.constant([[3.,4.],[5.,1.]])\n",
    "\n",
    "x1+x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e2032e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[ 3.,  8.],\n",
       "       [15.,  4.]], dtype=float32)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1*x2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2070cc",
   "metadata": {},
   "source": [
    "When adding/multiply tensors with different shapes, it will try to “broadcast”, by extending one by repetition to make it the same shape as the other. This only works if broadcasting can make them compatible, otherwise it throws up an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7921d970",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[1., 2.],\n",
       "       [6., 7.]], dtype=float32)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = tf. constant ([[1. ,2.] ,[5. ,6.]])\n",
    "x2 = tf. constant ([[0.] ,[1.]])\n",
    "\n",
    "#x3 is the \"broadcasted\" version of x2\n",
    "x3 = tf. constant ([[0. ,0.] ,[1. ,1.]])\n",
    "\n",
    "x1+x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e81a393c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[1., 2.],\n",
       "       [6., 7.]], dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1+x3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0777ed",
   "metadata": {},
   "source": [
    "Single-variable functions act componentwise, and are mostly the same as numpy, using tf.math. instead of np."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f92b3d4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[  2.7182817,   7.389056 ,  20.085537 ],\n",
       "       [ 54.59815  , 148.41316  , 403.4288   ]], dtype=float32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = tf.constant ([[1. ,2.,3.] ,[4.,5. ,6.]])\n",
    "tf.math.exp (x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79e53006",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[1.       , 1.4142135, 1.7320508],\n",
       "       [2.       , 2.236068 , 2.4494898]], dtype=float32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.math.sqrt (x1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80b4957",
   "metadata": {},
   "source": [
    "A few very useful, tensor-based commands are the following. We can add all elements, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb317325",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=21.0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_sum(x1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92d0388",
   "metadata": {},
   "source": [
    "reduce_sum can also sum over a particular axis. The axes are [0,1,...], in the same order as the shape [n1,n2,...]. Our tensor is [2,3], so setting axis=0 sums over the first, leaving a shape [3] tensor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ecb2841",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float32, numpy=array([5., 7., 9.], dtype=float32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_sum(x1,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0c9cd0",
   "metadata": {},
   "source": [
    "tf.reduce_mean does the same, but with averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52ebaa8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([2., 5.], dtype=float32)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_mean(x1,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9b4dca",
   "metadata": {},
   "source": [
    "A fundamental operation is taking the derivative of a tensor with respect to another. This happens both when defining the loss function itself (in PINNs-type approaches) and when finding gradients for optimisation. We use autodiff, which has two modes (forward and backwards). I'll focus on forward. To take the derivative, you have to record the function itself in a tf.GradientTape environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f61f09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 4.  8. 12.], shape=(3,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x2 = tf.constant([2.,4.,6])\n",
    "\n",
    "#Create the environment\n",
    "with tf.GradientTape() as t1:\n",
    "    \n",
    "    #Tell it to \"watch\" the variable with respect to which you want to take the derivative\n",
    "    t1.watch(x2)\n",
    "    \n",
    "    #Calculate the object of interest, this is y = x2[0]^2+x2[1]^2+x2[3]^2\n",
    "    y = tf.reduce_sum(x2**2)\n",
    "\n",
    "#Then take the derivative of y wrt x2, it should be 2*x2\n",
    "dy = t1.gradient(y,x2)\n",
    "print(dy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165d39a6",
   "metadata": {},
   "source": [
    "There are two main differences between constants and variables, one is how they behave in a GradientTape. Variables are automatically watched, constants are not. This is what happens if it doesn't \"watch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5842cb0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "#Create the environment\n",
    "with tf.GradientTape() as t1:\n",
    "    \n",
    "    #Calculate the object of interest, this is y = x2[0]^2+x2[1]^2+x2[3]^2\n",
    "    y = tf.reduce_sum(x2**2)\n",
    "\n",
    "#Then take the derivative of y wrt x2\n",
    "dy = t1.gradient(y,x2)\n",
    "print(dy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd87cccd",
   "metadata": {},
   "source": [
    "Now we try the same with a Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3026dd2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 4.  8. 12.], shape=(3,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x2 = tf.Variable([2.,4.,6.])\n",
    "\n",
    "#Create the environment\n",
    "with tf.GradientTape() as t1:\n",
    "    \n",
    "    #Calculate the object of interest, this is y = x2[0]^2+x2[1]^2+x2[3]^2\n",
    "    y = tf.reduce_sum(x2**2)\n",
    "\n",
    "#Then take the derivative of y wrt x2\n",
    "dy = t1.gradient(y,x2)\n",
    "print(dy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee409479",
   "metadata": {},
   "source": [
    "If the output and input tensors have the same shape, then gradient works componentwise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "566ada8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y= tf.Tensor([ 4. 16. 36.], shape=(3,), dtype=float32)\n",
      "grad =  tf.Tensor([ 4.  8. 12.], shape=(3,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with tf.GradientTape() as t1:\n",
    "    \n",
    "    #y is now x2**2 componentwise\n",
    "    y = x2**2\n",
    "    print(\"y=\", y)\n",
    "\n",
    "dy = t1.gradient(y,x2)\n",
    "print(\"grad = \", dy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160d2631",
   "metadata": {},
   "source": [
    "We can do higher order derivatives with nested GradientTapes. We will show how to do this with the Jacobian of the gradient (The Hessian). The command jacobian does what you would expect - with N dimensional input and output, it returns the NxN matrix of each output wrt each input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f5b77e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1. 0.]\n",
      " [0. 1.]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x1 = tf.Variable([2.,3.])\n",
    "\n",
    "with tf.GradientTape() as t1:\n",
    "    with tf.GradientTape() as t2:\n",
    "        y = tf.reduce_sum(x1**2)/2\n",
    "    dy = t2.gradient(y,x1)\n",
    "Hy = t1.jacobian(dy,x1)\n",
    "print(Hy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a539de0d-4884-4519-9a64-7e2f013227d4",
   "metadata": {},
   "source": [
    "If we have tensorial input -> scalar output, and we do computations componentwise, gradient works componentwise as well. Let's see an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "503874cd-a728-44ce-83f3-c7b722192d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z= tf.Tensor([ 5. 25. 61.], shape=(3,), dtype=float32)\n",
      "dz =  tf.Tensor(\n",
      "[[ 2.  4.]\n",
      " [ 6.  8.]\n",
      " [10. 12.]], shape=(3, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "xy = tf.constant([[1.,2.],[3.,4.],[5.,6.]])\n",
    "\n",
    "with tf.GradientTape() as t1:\n",
    "    t1.watch(xy)\n",
    "\n",
    "    #We will do the sums each element of xy^2. \n",
    "\n",
    "    z = tf.reduce_sum(xy**2,axis=1)\n",
    "    print(\"z=\", z)\n",
    "\n",
    "dz = t1.gradient(z,xy)\n",
    "print(\"dz = \", dz)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de570b76-a9c7-4004-80a5-c4f91ef5bcc8",
   "metadata": {},
   "source": [
    "The more \"complicated\" case is vectorial->vectorial, but still componentwise. We can have in mind a tensor of shape [N,3] mapping to one of shape [N,2], where \"N\" is the number of \"copies\" we want to consider. Our answer should be [N,2,3]. For this, we use batch_jacobian. I will use slicing to obtain the coordinates, and glue them together with \"stack\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2a1d971a-0f57-461d-8b06-ea5ce26d455a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[  2.   6.]\n",
      " [ 20.  30.]\n",
      " [ 56.  72.]\n",
      " [110. 132.]], shape=(4, 2), dtype=float32)\n",
      "Jacobian: tf.Tensor(\n",
      "[[[ 2.  1.  0.]\n",
      "  [ 0.  3.  2.]]\n",
      "\n",
      " [[ 5.  4.  0.]\n",
      "  [ 0.  6.  5.]]\n",
      "\n",
      " [[ 8.  7.  0.]\n",
      "  [ 0.  9.  8.]]\n",
      "\n",
      " [[11. 10.  0.]\n",
      "  [ 0. 12. 11.]]], shape=(4, 2, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "xyz = tf.constant([[1.,2.,3.],[4.,5.,6.],[7.,8.,9.],[10.,11.,12.]])\n",
    "\n",
    "with tf.GradientTape() as t1:\n",
    "    t1.watch(xyz)\n",
    "    x = xyz[:,0]\n",
    "    y = xyz[:,1]\n",
    "    z = xyz[:,2]\n",
    "    w1 = x*y\n",
    "    w2 = y*z\n",
    "    w = tf.stack([w1,w2],axis=-1)\n",
    "    print(w)\n",
    "\n",
    "jac_w = t1.batch_jacobian(w,xyz)\n",
    "print(\"Jacobian:\", jac_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e1e805",
   "metadata": {},
   "source": [
    "One potent tool in tensorflow is graph mode. By default, everything runs in \"Eager execution\" - typical line-by-line execution of code. The wrapper @tf.function turns it into \"Graph mode\", which is a more optimised version of the function. If the inputs stay the same shape, repeatedly running the same function is much faster (Iterative methods!!). Let's see how this works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a2918259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eager: 0.693925142288208\n",
      "Graph (First pass): 0.13084030151367188\n",
      "Graph (Second pass): 0.0565342903137207\n",
      "Graph (Third pass): 0.05708456039428711\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def Eager_function(x):\n",
    "    return tf.math.sin(x**2)\n",
    "\n",
    "@tf.function\n",
    "def Graph_function(x):\n",
    "    return tf.math.sin(x**2)\n",
    "\n",
    "x = tf.random.uniform([10**8])\n",
    "t0 = time.time()\n",
    "y = Eager_function(x)\n",
    "print(\"Eager:\",time.time()-t0)\n",
    "\n",
    "\n",
    "x = tf.random.uniform([10**8])\n",
    "t0 = time.time()\n",
    "y = Graph_function(x)\n",
    "print(\"Graph (First pass):\",time.time()-t0)\n",
    "\n",
    "\n",
    "x = tf.random.uniform([10**8])\n",
    "t0 = time.time()\n",
    "y = Graph_function(x)\n",
    "print(\"Graph (Second pass):\",time.time()-t0)\n",
    "\n",
    "\n",
    "x = tf.random.uniform([10**8])\n",
    "t0 = time.time()\n",
    "y = Graph_function(x)\n",
    "print(\"Graph (Third pass):\",time.time()-t0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8309121",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
